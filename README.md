<a id="readme-top"></a>

[![GitHub license](https://img.shields.io/badge/license-Apache%202.0-blue.svg)](https://github.com/Percena/MemFuse/blob/readme/LICENSE)

<!-- PROJECT LOGO -->
<br />
<div align="center">
  <a href="https://memfuse.vercel.app/">
    <img src="docs/assets/logo.png" alt="MemFuse Logo"
         style="max-width: 90%; height: auto; display: block; margin: 0 auto; padding-left: 16px; padding-right: 16px;">
  </a>
  <br />
  <br />

  <p align="center">
    <strong>MemFuse Core Services</strong>
    <br />
    The official core services for MemFuse, the open-source memory layer for LLMs.
    <br />
    <a href="https://memfuse.vercel.app/"><strong>Explore the Docs ¬ª</strong></a>
    <br />
    <br />
    <a href="https://memfuse.vercel.app/">View Demo</a>
    &middot;
    <a href="https://github.com/memfuse/memfuse/issues">Report Bug</a>
    &middot;
    <a href="https://github.com/memfuse/memfuse/issues">Request Feature</a>
  </p>
</div>

<!-- TABLE OF CONTENTS -->
<details>
  <summary>Table of Contents</summary>
  <ol>
    <li>
      <a href="#why-memfuse">Why MemFuse?</a>
    </li>
    <li>
      <a href="#key-features">Key Features</a>
    </li>
    <li><a href="#quick-start">Quick Start</a></li>
    <li><a href="#documentation">Documentation</a></li>
    <li><a href="#roadmap">Roadmap</a></li>
    <li><a href="#community-support">Community & Support</a></li>
  </ol>
</details>

## Why MemFuse?

Large-language-model apps are stateless out of the box.
Once the context window rolls over, yesterday's chat, the user's name, or that crucial fact vanishes.

**MemFuse** plugs a persistent, query-able memory layer between your LLM and a storage backend so agents can:

- remember user preferences across sessions
- recall facts & events thousands of turns later
- trim token spend instead of resending the whole chat history
- learn continuously and self-improve over time

## ‚ú® Key Features

| Category                          | What you get                                                                                                                     |
| --------------------------------- | -------------------------------------------------------------------------------------------------------------------------------- |
| **Lightning fast**                | Efficient buffering (write aggregation, intelligent prefetching, query caching) for rapid performance                            |
| **Unified Cognitive Search**      | Synergizes vector, graph, and keyword search with intelligent fusion & re-ranking for exceptional accuracy and diverse insights. |
| **Cognitive Memory Architecture** | Human-inspired layered memory: L0 (raw data/episodic), L1 (structured facts/semantic), and L2 (knowledge graph/conceptual).      |
| **Local-first**                   | Run the server locally or use Docker ‚Äî no mandatory cloud fees                                                                   |
| **Pluggable back-ends**           | Works with Chroma, Qdrant, pgvector, Neo4j, Redis, or any custom adapter (in progress)                                           |
| **Multi-tenant support**          | Secure isolation between users, agents, and sessions with built-in scoping and access controls                                   |
| **Framework-friendly**            | Drop-in providers for LangChain, AutoGen, Vercel AI SDK & raw OpenAI/Anthropic/Gemini/Ollama calls                               |
| **Apache 2.0**                    | Fully open source. Fork, extend, or ship as you like                                                                             |

---

## üöÄ Quick start

### Installation

First, ensure you have a MemFuse server running. To set up the MemFuse server locally:

1.  Clone the [main MemFuse repository](https://github.com/memfuse/memfuse):
    ```bash
    git clone https://github.com/memfuse/memfuse.git
    cd memfuse
    ```
2.  Once in the `memfuse` directory, install its dependencies and run the server using one of the following methods:

    **Using pip:**

    ```bash
    pip install -e .
    python -m memfuse_core
    ```

    **Or using Poetry:**

    ```bash
    poetry install
    poetry run memfuse-core
    ```

Then, install the MemFuse Python SDK:

```bash
pip install memfuse
```

For detailed installation instructions, configuration options, and troubleshooting tips, see the [Installation Guide](https://memfuse.vercel.app/docs/installation).

### Basic Usage

Here's a basic example of how to use the MemFuse Python SDK with OpenAI:

```python
from memfuse.llm import OpenAI
from memfuse import MemFuse
import os


memfuse_client = MemFuse(
  # api_key=os.getenv("MEMFUSE_API_KEY")
  # base_url=os.getenv("MEMFUSE_BASE_URL"),
)

memory = memfuse_client.init(
  user="alice",
  # agent="agent_default",
  # session=<randomly-generated-uuid>
)

# Initialize your LLM client with the memory scope
llm_client = OpenAI(
    api_key=os.getenv("OPENAI_API_KEY"),  # Your OpenAI API key
    memory=memory
)

# Make a chat completion request
response = llm_client.chat.completions.create(
    model="gpt-4o", # Or any model supported by your LLM provider
    messages=[{"role": "user", "content": "I'm planning a trip to Mars. What is the gravity there?"}]
)

print(f"Response: {response.choices[0].message.content}")
# Example Output: Response: Mars has a gravity of about 3.721 m/s¬≤, which is about 38% of Earth's gravity.
```

<!-- Ask a follow-up question. MemFuse automatically recalls relevant context. -->

Now, ask a follow-up question. MemFuse will automatically recall relevant context from the previous turn:

```python
# Ask a follow-up question. MemFuse automatically recalls relevant context.
followup_response = llm_client.chat.completions.create(
    model="gpt-4o",
    messages=[{"role": "user", "content": "What are some challenges of living on that planet?"}]
)

print(f"Follow-up: {followup_response.choices[0].message.content}")
# Example Output: Follow-up: Some challenges of living on Mars include its thin atmosphere, extreme temperatures, high radiation levels, and the lack of liquid water on the surface.
```

üî• That's it.
Every subsequent call under the same scope automatically writes notable facts to memory and fetches them when relevant.

---

## üìö Documentation

- [Installation Guide](https://memfuse.vercel.app/docs/installation): Detailed instructions for installing and configuring MemFuse
- [Getting Started](https://memfuse.vercel.app/docs/quickstart): Guide to using MemFuse in your projects
- [Examples](https://github.com/memfuse/memfuse-python/tree/main/examples): Sample code for chat-bots, autonomous agents, customer support, LangChain integration, etc.

---

## üõ£ Roadmap

### üì¶ Phase 1 ‚Äì MVP ("Fast & Transparent Core")

- [x] Lightning-fast‚ÄîEfficient buffering (write aggregation, intelligent prefetching, query caching) for rapid performance
- [x] Level 0 Memory Layer‚Äîraw chat history
- [x] Multi-tenant support
- [ ] Level 1 Memory Layer‚Äîsemantic/episodic memories
- [x] Re-ranking plugin‚ÄìLLM-powered memory scoring
- [x] Python SDK
- [x] Benchmarks: LongMemEval + MSC

### üß≠ Phase 2 ‚Äì Temporal Mastery & Quality

- [ ] JavaScript SDK
- [ ] Multimodal memory support
- [ ] Level 2 KG memory support
- [ ] Time-decay policies‚Äìautomatic forgetting of stale items

Have an idea? Open an issue or vote on the discussion board!

## ü§ù Community & Support

- GitHub Discussions: roadmap votes, RFCs, Q&A

If MemFuse saves you time, please ‚≠ê star the repo ‚Äî it helps the project grow!

## License

This MemFuse Core Services repo is licensed under the Apache 2.0 License. See the [LICENSE](LICENSE) file for more details.
(You'll need to add a LICENSE file to this repository, typically a copy of the Apache 2.0 license text).